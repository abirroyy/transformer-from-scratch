# ğŸ§  Transformer from Scratch â€” Reimplementation of â€œAttention Is All You Needâ€


## ğŸš€ Overview
This project is a complete, from-scratch implementation of the **Transformer architecture**, originally introduced in the 2017 paper *["Attention Is All You Need"](https://arxiv.org/abs/1706.03762)*.  
The goal was to deeply understand how attention mechanisms, positional encoding, and sequence-to-sequence learning works.

---

## ğŸ§© Key Features
- **Implemented Multi-Head Self-Attention** from first principles  
- **Positional Encoding** (sinusoidal) for sequence order awareness  
- **Encoderâ€“Decoder Architecture** following the original paper design  
- **Layer Normalization, Residual Connections, and Masking**  
- **Custom Training Loop** for sequence-to-sequence tasks  

---

## ğŸ§¬ Architecture Summary
The Transformer consists of:
1. **Encoder Stack**
   - Multi-Head Self-Attention
   - Feed Forward Network
   - Add & Norm Layers  
2. **Decoder Stack**
   - Masked Multi-Head Self-Attention
   - Encoderâ€“Decoder Attention
   - Feed Forward + Add & Norm  
3. **Output Projection**
   - Linear layer projecting to vocabulary logits  
   - Softmax activation for probabilities

---

## Transformer Architecture forward pass output

<p align="center">
  <img src="Selection_002.png" alt="Transformer Architecture forward pass output" width="600"/>
</p>





